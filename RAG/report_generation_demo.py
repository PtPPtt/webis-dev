"""
Improved Report Generation Demo

This demo showcases the refined report generation with:
- Concise information extraction (no verbosity)
- Source attribution for every piece of information
- LLM-powered synthesis with structure guidance
- Top-N document focus (not all documents)

Features:
- Explicit source tracking: ã€æ¥æºï¼šæ–‡æ¡£åã€‘
- Minimal redundancy
- LLM-guided extraction through precise prompts
- Multiple demonstration scenarios

Run:
  python RAG/demo_improved_report.py
"""
from __future__ import annotations

import os
from pathlib import Path
from rag_pipeline import RAGPipeline
from rag_tasks import TaskPipeline, ReportGenerationTask


def get_openai_llm():
    """Initialize OpenAI LLM client"""
    try:
        from langchain_openai import ChatOpenAI
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âš ï¸  OPENAI_API_KEY not set, running without LLM")
            return None
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=api_key,
        )
        print("âœ“ Using OpenAI GPT-4o-mini")
        return llm
    except ImportError:
        print("âš ï¸  langchain-openai not installed")
        return None


def demo_1_concise_report():
    """Demo 1: Generate concise report with source attribution"""
    print("\n" + "=" * 70)
    print("DEMO 1: ç²¾ç‚¼æŠ¥å‘Š - å¸¦æ¥æºæ ‡æ³¨")
    print("=" * 70)
    
    llm = get_openai_llm()
    
    rag_pipeline = RAGPipeline(
        min_doc_threshold=4,
        min_score_threshold=0.4,
    )
    
    query = "tell me the recent development of artificial intelligence"
    print(f"\nğŸ“Œ æŸ¥è¯¢: {query}\n")
    
    # Get context
    print("ç¬¬1æ­¥ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    context = rag_pipeline.get_retrieval_context(query, auto_fetch_webis=True, top_k=5)
    print(f"âœ“ æ£€ç´¢åˆ° {context['metadata']['retrieval_count']} ç¯‡æ–‡æ¡£\n")
    
    # Generate report
    print("ç¬¬2æ­¥ï¼šç”Ÿæˆç²¾ç‚¼æŠ¥å‘Šï¼ˆå«æ¥æºï¼‰...")
    task_pipeline = TaskPipeline()
    task_pipeline.add_task(ReportGenerationTask(
        llm=llm,
        include_raw_data=False,  # Only concise info
        output_format="markdown"
    ))
    
    result = task_pipeline.execute(context)
    
    if result['task_results'][0]['success']:
        report_path = result['task_results'][0]['output_path']
        print(f"âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ")
        print(f"  æ–‡ä»¶: {Path(report_path).name}")
        print(f"  å†…å®¹é•¿åº¦: {result['task_results'][0]['stats']['content_length']} å­—ç¬¦")
        
        # Show report preview
        print(f"\nğŸ“„ æŠ¥å‘Šé¢„è§ˆ:")
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
            lines = content.split('\n')
            for line in lines[:30]:  # Show first 30 lines
                print(line)
            if len(lines) > 30:
                print(f"... (è¿˜æœ‰ {len(lines) - 30} è¡Œ)")


def demo_2_source_tracking():
    """Demo 2: Demonstrate source tracking in all sections"""
    print("\n" + "=" * 70)
    print("DEMO 2: æ¥æºè¿½è¸ª - æ¯æ¡ä¿¡æ¯éƒ½æ ‡æ³¨å‡ºå¤„")
    print("=" * 70)
    
    llm = get_openai_llm()
    
    rag_pipeline = RAGPipeline(
        min_doc_threshold=1,
        min_score_threshold=0.2,
    )
    
    query = "äº‘è®¡ç®—çš„å‘å±•è¶‹åŠ¿"
    print(f"\nğŸ“Œ æŸ¥è¯¢: {query}\n")
    
    # Get context
    print("æ£€ç´¢ä¸­...")
    context = rag_pipeline.get_retrieval_context(query, auto_fetch_webis=True)
    print(f"âœ“ æ£€ç´¢åˆ° {context['metadata']['retrieval_count']} ç¯‡æ–‡æ¡£\n")
    
    # Generate report with source tracking
    print("ç”Ÿæˆå¸¦æ¥æºè¿½è¸ªçš„æŠ¥å‘Š...")
    task_pipeline = TaskPipeline()
    task_pipeline.add_task(ReportGenerationTask(
        llm=llm,
        include_raw_data=False,
        output_format="markdown"
    ))
    
    result = task_pipeline.execute(context)
    
    if result['task_results'][0]['success']:
        report_path = result['task_results'][0]['output_path']
        
        # Read and display sections with source tracking
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print("\næå–çš„å…³é”®å‘ç°ï¼ˆå¸¦æ¥æºæ ‡æ³¨ï¼‰:")
        print("-" * 70)
        
        in_findings = False
        for line in content.split('\n'):
            if "## å…³é”®å‘ç°" in line:
                in_findings = True
                continue
            if in_findings and line.startswith("## "):
                break
            if in_findings and line.strip().startswith("- ã€æ¥æº"):
                print(line)


def demo_3_comparison_with_without_llm():
    """Demo 3: Compare report with and without LLM"""
    print("\n" + "=" * 70)
    print("DEMO 3: å¯¹æ¯” - æœ‰LLM vs æ— LLM")
    print("=" * 70)
    
    llm = get_openai_llm()
    
    rag_pipeline = RAGPipeline(
        min_doc_threshold=5,
        min_score_threshold=0.4,
    )
    
    query = "åŒºå—é“¾æŠ€æœ¯åº”ç”¨"
    print(f"\nğŸ“Œ æŸ¥è¯¢: {query}\n")
    
    context = rag_pipeline.get_retrieval_context(query, auto_fetch_webis=True, top_k=5)
    
    # Report without LLM
    print("ç”ŸæˆæŠ¥å‘Š (ä¸ä½¿ç”¨LLM)...")
    task_pipeline_no_llm = TaskPipeline()
    task_pipeline_no_llm.add_task(ReportGenerationTask(
        llm=None,
        include_raw_data=False,
        output_format="markdown"
    ))
    result_no_llm = task_pipeline_no_llm.execute(context)
    
    if result_no_llm['task_results'][0]['success']:
        size_no_llm = result_no_llm['task_results'][0]['stats']['content_length']
        print(f"âœ“ æ— LLMæŠ¥å‘Š: {size_no_llm} å­—ç¬¦")
    
    # Report with LLM
    if llm:
        print("ç”ŸæˆæŠ¥å‘Š (ä½¿ç”¨LLM)...")
        task_pipeline_llm = TaskPipeline()
        task_pipeline_llm.add_task(ReportGenerationTask(
            llm=llm,
            include_raw_data=False,
            output_format="markdown"
        ))
        result_llm = task_pipeline_llm.execute(context)
        
        if result_llm['task_results'][0]['success']:
            size_llm = result_llm['task_results'][0]['stats']['content_length']
            print(f"âœ“ ä½¿ç”¨LLMæŠ¥å‘Š: {size_llm} å­—ç¬¦")
            print(f"\nå¯¹æ¯”:")
            print(f"  - æœ‰LLM: æ›´ç²¾ç‚¼ã€æ›´ç»“æ„åŒ–")
            print(f"  - æ— LLM: ç®€å•æå–ã€å¯èƒ½æœ‰å†—ä½™")


def demo_4_batch_generation():
    """Demo 4: Batch generation with source tracking"""
    print("\n" + "=" * 70)
    print("DEMO 4: æ‰¹é‡ç”Ÿæˆ - å¤šæŸ¥è¯¢æ‰¹å¤„ç†")
    print("=" * 70)
    
    llm = get_openai_llm()
    
    rag_pipeline = RAGPipeline(
        min_doc_threshold=1,
        min_score_threshold=0.2,
    )
    
    queries = [
        "5GæŠ€æœ¯å‘å±•",
        "é‡å­è®¡ç®—",
        "è¾¹ç¼˜è®¡ç®—",
    ]
    
    print("\nç”Ÿæˆå¤šä¸ªæŠ¥å‘Š...\n")
    
    for i, query in enumerate(queries, 1):
        print(f"{i}. å¤„ç†: {query}")
        
        try:
            context = rag_pipeline.get_retrieval_context(query, auto_fetch_webis=True)
            
            task_pipeline = TaskPipeline()
            task_pipeline.add_task(ReportGenerationTask(
                llm=llm,
                include_raw_data=False,
                output_format="markdown"
            ))
            
            result = task_pipeline.execute(context)
            
            if result['task_results'][0]['success']:
                report_path = result['task_results'][0]['output_path']
                size = result['task_results'][0]['stats']['content_length']
                print(f"   âœ“ ç”Ÿæˆ: {Path(report_path).name} ({size} å­—ç¬¦)")
            else:
                print(f"   âœ— å¤±è´¥: {result['task_results'][0].get('error')}")
        
        except Exception as e:
            print(f"   âœ— é”™è¯¯: {e}")


def main():
    """Run all demos"""
    print("\n" + "=" * 70)
    print("ç²¾ç‚¼æŠ¥å‘Šç”Ÿæˆæ¼”ç¤º - å¸¦æ¥æºæ ‡æ³¨")
    print("=" * 70)
    print("\nå±•ç¤ºæ”¹è¿›çš„æŠ¥å‘Šç”ŸæˆåŠŸèƒ½ï¼š")
    print("  âœ“ ç²¾ç‚¼çš„ä¿¡æ¯æå–")
    print("  âœ“ æ¥æºæ ‡æ³¨ï¼šã€æ¥æºï¼šæ–‡æ¡£åã€‘")
    print("  âœ“ æ— å†—ä½™")
    print("  âœ“ LLMå¢å¼ºçš„ç»“æ„åŒ–")
    print()
    
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  æç¤º: è®¾ç½® OPENAI_API_KEY å¯è·å¾—æ›´å¥½çš„LLMé©±åŠ¨çš„æŠ¥å‘Š")
        print("   export OPENAI_API_KEY='sk-...'\n")
    
    try:
        demo_1_concise_report()
        # demo_2_source_tracking()
        # demo_3_comparison_with_without_llm()
        # demo_4_batch_generation()
        
        print("\n" + "=" * 70)
        print("âœ“ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ")
        print("=" * 70)
        print("\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³ ./data/ ç›®å½•")
        print("\nğŸ’¡ æ”¹è¿›è¦ç‚¹:")
        print("  - ä¿¡æ¯ç²¾ç‚¼ï¼Œæ— å†—ä½™")
        print("  - æ¯æ¡ä¿¡æ¯éƒ½æœ‰æ¥æºæ ‡æ³¨")
        print("  - LLMç”Ÿæˆçš„æç¤ºè¯æŒ‡å¯¼ç»“æ„åŒ–è¾“å‡º")
        print("  - æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆMarkdown/PDFï¼‰")
        print()
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
